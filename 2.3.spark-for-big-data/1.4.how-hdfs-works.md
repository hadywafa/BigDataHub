# ğŸ“‚ **How HDFS Splits Large Files into Chunks for Storage** ğŸ”

Before diving deeper into **Apache Spark**, letâ€™s first understand how **HDFS (Hadoop Distributed File System)** **splits large files into chunks** and distributes them across multiple nodes.

This process is **fundamental** to how **Big Data is stored efficiently in a distributed system**.

---

## ğŸ— **1. Why Does HDFS Split Files into Chunks?**

A **single machine cannot store petabytes of data**, so HDFS **breaks files into blocks** and stores them **across multiple machines** in a cluster.

### ğŸ”¹ **Key Reasons for Splitting Files into Blocks:**

- âœ” **Scalability** â€“ Large files can be stored across multiple machines.
- âœ” **Fault Tolerance** â€“ If one node fails, the same block exists on other nodes.
- âœ” **Parallel Processing** â€“ Different parts of the file can be processed simultaneously.

> ğŸ“Œ **By default, HDFS splits files into** **128MB (or 256MB) blocks** and stores them across multiple nodes.

---

## ğŸ” **2. Who Splits the File?**

When a file is **uploaded to HDFS**, the **HDFS Client** performs the following tasks:

- 1ï¸âƒ£ **Breaks the file into fixed-size blocks (default: 128MB).**
- 2ï¸âƒ£ **Sends each block to different DataNodes** for storage.
- 3ï¸âƒ£ **Stores metadata (block location) in the NameNode.**

### ğŸ— **Key Players in the File Splitting Process**

| Component       | Role                                           |
| --------------- | ---------------------------------------------- |
| **HDFS Client** | Splits the file into chunks before storing it. |
| **NameNode**    | Stores metadata (file-to-block mapping).       |
| **DataNodes**   | Store the actual data blocks.                  |

---

## ğŸ”„ **3. Step-by-Step Process of File Splitting in HDFS**

Letâ€™s say we have a **300MB file** named `bigdata.log` that we want to store in HDFS.

ğŸ“‚ **Step 1: HDFS Client Splits the File**

- The **HDFS Client** checks the file size (`300MB`).
- Since the default **HDFS block size** is `128MB`, the file is divided into:
  - **Block 1** â†’ `128MB`
  - **Block 2** â†’ `128MB`
  - **Block 3** â†’ `44MB` (remaining data)

ğŸ“‚ **Step 2: Blocks Are Sent to Different DataNodes**

- HDFS **randomly assigns each block** to different DataNodes in the cluster.
- Each block is **replicated** (default: 3 copies) for **fault tolerance**.

ğŸ“‚ **Step 3: Metadata is Stored in NameNode**

- The **NameNode** keeps track of **which DataNode stores which block**.
- No actual file data is stored in the NameNodeâ€”**only metadata** (file-to-block mapping).

---

## ğŸ“Š **4. File Splitting and Storage in HDFS (Sequence Diagram)**

```mermaid
sequenceDiagram
    participant User as User
    participant HDFSClient as HDFS Client
    participant NameNode as NameNode (Metadata Manager)
    participant DataNode1 as DataNode 1
    participant DataNode2 as DataNode 2
    participant DataNode3 as DataNode 3

    User->>HDFSClient: Upload file "bigdata.log" (300MB)
    HDFSClient->>HDFSClient: Split file into 128MB, 128MB, and 44MB blocks
    HDFSClient->>NameNode: Request block storage locations
    NameNode->>HDFSClient: Assign DataNodes for each block

    HDFSClient->>DataNode1: Store Block 1 (128MB)
    HDFSClient->>DataNode2: Store Block 2 (128MB)
    HDFSClient->>DataNode3: Store Block 3 (44MB)

    DataNode1->>NameNode: Report "Block 1 stored"
    DataNode2->>NameNode: Report "Block 2 stored"
    DataNode3->>NameNode: Report "Block 3 stored"

    NameNode->>User: File successfully stored in HDFS!
```

---

## ğŸ”„ **5. Replication for Fault Tolerance**

HDFS does **not store a block on just one machine**â€”it **replicates each block** across the cluster.

| Block           | Primary DataNode | Replica 1  | Replica 2  |
| --------------- | ---------------- | ---------- | ---------- |
| Block 1 (128MB) | DataNode 1       | DataNode 2 | DataNode 3 |
| Block 2 (128MB) | DataNode 2       | DataNode 3 | DataNode 4 |
| Block 3 (44MB)  | DataNode 3       | DataNode 4 | DataNode 1 |

ğŸ’¡ **This ensures that if a DataNode fails, HDFS can still retrieve the data from another copy!**

---

## ğŸ **6. Summary of the Process**

- âœ” **The HDFS Client splits large files into 128MB (or 256MB) blocks.**
- âœ” **Each block is stored on different DataNodes in the cluster.**
- âœ” **The NameNode keeps track of block locations.**
- âœ” **Blocks are replicated across multiple DataNodes for fault tolerance.**
