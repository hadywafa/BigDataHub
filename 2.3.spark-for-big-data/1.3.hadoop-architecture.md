# 🏗 **Hadoop Architecture - Core Components & Execution Workflow**

Understanding **Hadoop’s execution workflow** is essential to see **how data flows through the system**. Hadoop follows a **Master-Slave architecture**, where the **Master nodes** manage the system, and the **Slave nodes** execute tasks.

Hadoop processing can be divided into **two main scenarios**:

- 1️⃣ **Storage Workflow** (When a file is stored in HDFS)
- 2️⃣ **Batch Processing Workflow** (When a user runs a query or submits a job)

---

<div style="display: flex; justify-content: center; align-items: center;">
    <img alt="Hadoop Architecture" src="images/hadoop-architecture.png" />
</div>

---

## 🔥 **Hadoop Architecture - The Big Picture**

```mermaid
graph TD
  A[👤 User/💻 HDFS Client]

  subgraph X[Hadoop Master Nodes]
    direction TB
    B[🐝 External Tools - Hive, Pig, HBase, Spark]
    C[🎛 ResourceManager - YARN]
    D[🧠 NameNode - HDFS]
    E[📜 Secondary NameNode - HDFS]
    F[📝 ApplicationMaster - YARN]
    G[🧠 JobTracker - MapReduce]
  end

  subgraph Z[Hadoop Slave Nodes]
    direction TB
    H[💾 DataNodes - HDFS]
    I[🖥️ NodeManagers - YARN]
    J[🔄 TaskTrackers - MapReduce]
  end


  A --> X --> Z

```

### **📌 Detailed Explanation:**

1. **Master Nodes**:

   - **NameNode (🧠)**: Manages metadata about the file system and locations of data blocks.
   - **Secondary NameNode (📜)**: Takes periodic snapshots of the NameNode's metadata but does not serve as a backup.
   - **ResourceManager (🎛)**: Central YARN component that allocates resources and manages the execution of distributed applications.
   - **ApplicationMaster (📝)**: Manages the lifecycle of applications, negotiates resources, and oversees task execution.
   - **External Tools (🐝)**: Tools like Hive (for SQL processing), Pig (for scripting), and HBase (for NoSQL database) interact with the Hadoop ecosystem to provide additional functionality.

2. **Slave Nodes**:
   - **DataNodes (💾)**: Store the actual data blocks. Handle read and write requests from clients and perform block replication.
   - **NodeManagers (🖥️)**: Run tasks on worker nodes and report resource usage to the ResourceManager.
   - **TaskTrackers (🔄)**: Execute individual tasks assigned by the MapReduce JobTracker.

### **📌 Key Components:**

#### **HDFS Components**

- **NameNode (🧠)**: Manages metadata and file locations in HDFS.
- **DataNode (💾)**: Stores actual data blocks in HDFS.
- **Secondary NameNode (📜)**: Takes periodic snapshots of the NameNode's metadata.

#### **MapReduce Components**

- **JobTracker**: Manages job scheduling and task assignment (not shown in the diagram).
- **TaskTracker (🔄)**: Executes tasks assigned by the JobTracker.

#### **YARN Components**

- **ResourceManager (🎛)**: Allocates resources and manages the execution of distributed applications.
- **NodeManager (🖥️)**: Manages resources and application containers on each node.
- **ApplicationMaster (📝)**: Manages the lifecycle of applications, including resource negotiation and task execution.

## 📂 **Storage Workflow in HDFS (Step-by-Step)**

```mermaid
sequenceDiagram
    participant User as 👤 User
    participant HDFSClient as 🗄️ HDFS Client (Master)
    participant NameNode as 🧠 NameNode - Metadata (Master)
    participant DataNode1 as 💾 DataNode 1 (Slave)
    participant DataNode2 as 💾 DataNode 2 (Slave)
    participant DataNode3 as 💾 DataNode 3 (Slave)

    User->>HDFSClient: Upload file "bigdata.log" (300MB)
    HDFSClient->>HDFSClient: Split file into 128MB, 128MB, and 44MB blocks
    HDFSClient->>NameNode: Request block storage locations
    NameNode->>HDFSClient: Assign DataNodes for each block

    HDFSClient->>DataNode1: Store Block 1 (128MB)
    HDFSClient->>DataNode2: Store Block 2 (128MB)
    HDFSClient->>DataNode3: Store Block 3 (44MB)

    DataNode1->>NameNode: Report "Block 1 stored"
    DataNode2->>NameNode: Report "Block 2 stored"
    DataNode3->>NameNode: Report "Block 3 stored"

    NameNode->>User: File successfully stored in HDFS!
```

📌 **Explanation:**

- 1️⃣ The **HDFS Client** splits large files into blocks.
- 2️⃣ The **NameNode (Master)** assigns DataNodes for storage.
- 3️⃣ **DataNodes (Slaves)** store the actual data blocks.
- 4️⃣ The **NameNode tracks the metadata** (which block is stored where).

💡 **HDFS ensures data redundancy by replicating blocks across multiple nodes.**

---

## 🏗 **Batch Processing Workflow in Hadoop (Step-by-Step)**

```mermaid
sequenceDiagram
    participant 👤 User
    participant 🐝 ExternalTools (Master)
    participant 🎛 ResourceManager (Master)
    participant 📝 ApplicationMaster (Master)
    participant 🖥️ NodeManager (Slave)
    participant 🔄 TaskTracker (Slave)
    participant 💾 DataNode (Slave)

    note over 👤 User: Submits job/query
    👤 User->>🐝 ExternalTools (Master): Submit query (e.g., Hive, Pig)
    note over 🐝 ExternalTools (Master): Converts query to job
    🐝 ExternalTools (Master)->>🎛 ResourceManager (Master): Request resources for job
    note over 🎛 ResourceManager (Master): Allocates resources
    🎛 ResourceManager (Master)-->>🐝 ExternalTools (Master): Resources allocated
    🐝 ExternalTools (Master)->>📝 ApplicationMaster (Master): Submit application
    note over 📝 ApplicationMaster (Master): Manages application lifecycle
    📝 ApplicationMaster (Master)->>🎛 ResourceManager (Master): Negotiate resources
    🎛 ResourceManager (Master)-->>📝 ApplicationMaster (Master): Allocate containers
    📝 ApplicationMaster (Master)->>🖥️ NodeManager (Slave): Request container launch
    note over 🖥️ NodeManager (Slave): Launches containers
    🖥️ NodeManager (Slave)-->>📝 ApplicationMaster (Master): Container launched

    note over 📝 ApplicationMaster (Master): Assigns tasks to TaskTracker
    📝 ApplicationMaster (Master)->>🔄 TaskTracker (Slave): Assign tasks
    🔄 TaskTracker (Slave)->>💾 DataNode (Slave): Process data blocks
    note over 💾 DataNode (Slave): Provides data blocks to TaskTracker
    💾 DataNode (Slave)-->>🔄 TaskTracker (Slave): Provide data blocks
    🔄 TaskTracker (Slave)-->>📝 ApplicationMaster (Master): Task completion status
    note over 📝 ApplicationMaster (Master): Monitors task completion
    📝 ApplicationMaster (Master)-->>🐝 ExternalTools (Master): Job completion status
    🐝 ExternalTools (Master)-->>👤 User: Query results
```

### **Processing Workflow (Master/Slave)**

1. **User (👤)**: Submits a job or query through external tools like Hive or Pig.
2. **External Tools (Master) (🐝)**: Convert the query into a job and request resources from the ResourceManager.
3. **ResourceManager (Master) (🎛)**: Allocates resources and informs the External Tools.
4. **External Tools (Master) (🐝)**: Submit the application to the ApplicationMaster.
5. **ApplicationMaster (Master) (📝)**: Manages the application's lifecycle, negotiates resources, and requests container launch from NodeManagers.
6. **NodeManager (Slave) (🖥️)**: Launches the containers and informs the ApplicationMaster.
7. **ApplicationMaster (Master) (📝)**: Assigns tasks to TaskTrackers.
8. **TaskTracker (Slave) (🔄)**: Processes data blocks by interacting with DataNodes and provides the task completion status to the ApplicationMaster.
9. **ApplicationMaster (Master) (📝)**: Provides the job completion status to the External Tools.
10. **External Tools (Master) (🐝)**: Return the query results to the User.
