# ğŸ§¸ **Hadoop - The Grandfather of Big Data Processing**

Before diving deep into **Apache Spark**, it's essential to understand **Hadoop**. Why? Because **Spark was created to overcome Hadoop's limitations**, and many Spark jobs still interact with Hadoop systems (like HDFS).

Let's **unravel the mystery of Hadoop** in a way that actually makes sense! ğŸ˜

---

<div style="text-align: center;">
    <img src="images/hadoop.png" alt="Hadoop Logo" />
</div>

---

## ğŸ”¥ **1. What is Hadoop, and Why Was It Created?**

### ğŸ“Œ **The Simple Definition**

Hadoop is an **open-source framework** that allows storing and processing **huge amounts of data** across many computers in a **fault-tolerant and parallel** manner.

### ğŸ§  **The Problem Before Hadoop**

In the early 2000s, organizations faced a data explosion:

- **Millions of website visits daily** ğŸ“Š
- **Massive log files from web applications** ğŸ“
- **Huge datasets from e-commerce, banking, and IoT** ğŸŒ

ğŸš¨ **Challenges:**

- âŒ **Traditional Databases (SQL, MySQL, Oracle) struggled** with large datasets.
- âŒ **Single servers couldn't store and process petabytes of data efficiently.**
- âŒ **Data processing took hours or even days.**

ğŸ’¡ Hadoop **solved all these problems** by introducing **distributed storage** and **parallel processing**. ğŸ’¡

### ğŸ”‘ **Hadoop Key Features**

Hadoop became the **foundation of modern Big Data** because it:

- âœ… **Stores data across multiple machines** (scalability).
- âœ… **Processes data in parallel** (efficiency).
- âœ… **Handles machine failures automatically** (fault tolerance).

---

## ğŸ› **2. Hadoop Architecture - The Core Components**

Hadoop is **not just one tool**; it consists of **multiple components** working together.

### ğŸ— **The Three Pillars of Hadoop**

| ğŸ› **Component**                              | ğŸš€ **Purpose**                                    |
| -------------------------------------------- | ------------------------------------------------- |
| **HDFS (Hadoop Distributed File System)** ğŸ“‚ | Stores large-scale data across multiple machines. |
| **MapReduce** ğŸ­                             | Processes data in parallel (but slow).            |
| **YARN (Yet Another Resource Negotiator)** ğŸ› | Manages cluster resources efficiently.            |

Each component plays a crucial role in **handling petabytes of data** in a distributed environment.

---

## ğŸ“‚ **3. HDFS (Hadoop Distributed File System) - Storage Layer**

HDFS is **Hadoopâ€™s storage system**. Instead of storing one massive file on a single machine, it **splits data into blocks and distributes them** across multiple machines.

### ğŸ”¹ **Key Features of HDFS**

âœ” **Distributed Storage** â€“ Splits large files into chunks.  
âœ” **Fault Tolerance** â€“ Replicates data across nodes to prevent loss.  
âœ” **Scalability** â€“ Handles petabytes of data efficiently.

### ğŸ›  **Key HDFS Components**

| Component                 | Role                                     |
| ------------------------- | ---------------------------------------- |
| **NameNode** ğŸ§            | Manages metadata and file locations.     |
| **DataNode** ğŸ’¾           | Stores actual data blocks.               |
| **Secondary NameNode** ğŸ“œ | Keeps metadata snapshots (not a backup). |

### ğŸ— **How HDFS Works**

- 1ï¸âƒ£ A **file is split into blocks** (default size: 128MB or 256MB).
- 2ï¸âƒ£ Each **block is stored on different nodes** in the cluster.
- 3ï¸âƒ£ **HDFS replicates blocks** (default: 3 copies) to prevent data loss.
- 4ï¸âƒ£ If a machine fails, **another copy of the block is used** for recovery.

---

```mermaid
sequenceDiagram
    participant ğŸ‘¤ User
    participant ğŸ’» HDFS Client (User/Client)
    participant ğŸ§  NameNode (Master)
    participant ğŸ’¾ DataNode (Slave)

    note over ğŸ‘¤ User: Initiates data storage request
    ğŸ‘¤ User->>ğŸ’» HDFS Client (User/Client): Submit data to HDFS
    note over ğŸ’» HDFS Client (User/Client): Splits file into blocks
    ğŸ’» HDFS Client (User/Client)->>ğŸ§  NameNode (Master): Request file system metadata
    note over ğŸ§  NameNode (Master): Provides metadata and block locations
    ğŸ§  NameNode (Master)-->>ğŸ’» HDFS Client (User/Client): Metadata and block locations
    note over ğŸ’» HDFS Client (User/Client): Uploads blocks to DataNodes
    ğŸ’» HDFS Client (User/Client)->>ğŸ’¾ DataNode (Slave): Upload data blocks
    note over ğŸ’¾ DataNode (Slave): Stores blocks and replicates
    ğŸ’¾ DataNode (Slave)-->>ğŸ’» HDFS Client (User/Client): Acknowledge block storage
    note over ğŸ’» HDFS Client (User/Client): Confirms completion to User
    ğŸ’» HDFS Client (User/Client)-->>ğŸ‘¤ User: Data storage complete
```

1. **User (ğŸ‘¤)**: Initiates the request to store data in HDFS.
2. **HDFS Client (User/Client) (ğŸ’»)**: Splits the file into blocks and communicates with the NameNode to get metadata and block locations.
3. **NameNode (Master) (ğŸ§ )**: Provides the metadata and block locations to the HDFS Client.
4. **HDFS Client (User/Client) (ğŸ’»)**: Uploads the data blocks to DataNodes.
5. **DataNode (Slave) (ğŸ’¾)**: Stores the data blocks and replicates them as necessary.
6. **HDFS Client (User/Client) (ğŸ’»)**: Acknowledges the completion of data storage to the User.

ğŸš¨ **Why HDFS Matters:** It ensures **data availability, redundancy, and reliability** even if machines fail.

---

## ğŸ­ **4. MapReduce - Processing Layer**

MapReduce is **Hadoopâ€™s original processing engine**, designed to run computations in parallel across a cluster. However, it has **performance issues** (which weâ€™ll discuss shortly).

### ğŸ“ **How MapReduce Works (Step-by-Step)**

1ï¸âƒ£ **Map Phase** â€“ Breaks data into key-value pairs.  
2ï¸âƒ£ **Shuffle Phase** â€“ Groups similar keys together.  
3ï¸âƒ£ **Reduce Phase** â€“ Aggregates and processes the final result.

### ğŸ“Š **Example: Word Count Program**

<div style="text-align: center;">
    <img src="images/hadoop-map-reduce-example.png" alt="Hadoop MapReduce example" />
</div>

```mermaid
sequenceDiagram
    participant Input
    participant Mapper
    participant ShuffleSort
    participant Reducer
    participant Output

    Input->>Mapper: "Hadoop is slow, Hadoop is powerful"
    Mapper->>ShuffleSort: ("Hadoop",1), ("is",1), ("slow",1), ("Hadoop",1), ("is",1), ("powerful",1)
    ShuffleSort->>Reducer: ("Hadoop",[1,1]), ("is",[1,1]), ("slow",[1]), ("powerful",[1])
    Reducer->>Output: ("Hadoop",2), ("is",2), ("slow",1), ("powerful",1)
```

### âš  **Why is MapReduce Slow?**

- âŒ **Disk-Based Processing** â€“ Every stage reads/writes to disk, causing delays.
- âŒ **Batch-Only Processing** â€“ No support for real-time analytics.
- âŒ **Java-Based** â€“ Requires **a lot of boilerplate code**.

ğŸ’¡ **SOLUTION:** Apache Spark **keeps everything in memory** for much faster performance! âš¡

---

## ğŸ› **5. YARN - Resource Management Layer**

YARN (Yet Another Resource Negotiator) **allocates computing resources** across different Hadoop jobs.

### ğŸ— **Key YARN Components**

| Component                 | Role                                           |
| ------------------------- | ---------------------------------------------- |
| **Resource Manager** ğŸ¯   | Allocates CPU and memory for tasks.            |
| **Node Manager** ğŸ’»       | Runs on each machine to monitor jobs.          |
| **Application Master** ğŸ† | Manages a **single job** from start to finish. |

### ğŸ’¡ **Why YARN Matters**

If you're using **pure Java**, you write MapReduce jobs directly.  
If you're using **Hive (SQL on Hadoop)**, YARN **converts SQL into Java MapReduce functions** automatically.

âŒ **So, if you donâ€™t want to write Java for everything, you need tools like Hive!**

---

## âš  **6. Limitations of Hadoop (Why Itâ€™s Not Enough!)**

### âŒ **1. Slow Performance** ğŸ¢

- MapReduce **relies on disk storage** instead of in-memory processing.
- Disk I/O causes **significant performance bottlenecks**.

### âŒ **2. No Real-Time Processing** â³

- Hadoop is **batch-based**â€”not designed for real-time applications.
- Example: **Fraud detection in banking requires instant decisions**.

### âŒ **3. Java-Centric Complexity** ğŸ“œ

- Writing **MapReduce jobs requires Java** and is not user-friendly.
- Even Hive and Pig still generate **Java-based execution plans**.

### âŒ **4. Expensive Cluster Management** ğŸ’°

- Requires **more hardware** to maintain performance.
- Spark does more with **fewer resources**.

---

## ğŸš€ **7. How Apache Spark Fixes Hadoopâ€™s Problems**

Spark was **designed to improve upon Hadoop** while remaining compatible with HDFS.

| Feature              | Hadoop (MapReduce)      | Apache Spark                     |
| -------------------- | ----------------------- | -------------------------------- |
| **Speed**            | Slow (disk-based)       | 100x faster (in-memory)          |
| **Processing**       | Batch only              | Batch + Real-time                |
| **Ease of Use**      | Java-based              | Simple APIs (Python, Scala, SQL) |
| **Fault Tolerance**  | High (data replication) | High (RDDs & lineage)            |
| **Machine Learning** | None                    | Built-in MLlib library           |

---

## ğŸ **8. Summary**

- âœ… **Hadoop introduced scalable Big Data storage and processing.**
- âœ… **HDFS provides fault-tolerant storage.**
- âœ… **MapReduce enables distributed processing but is slow.**
- âœ… **YARN manages cluster resources but executes Java functions.**
- âœ… **Hadoop lacks real-time capabilities and requires extensive Java code.**
- âœ… **Apache Spark was built to address these issues.**
